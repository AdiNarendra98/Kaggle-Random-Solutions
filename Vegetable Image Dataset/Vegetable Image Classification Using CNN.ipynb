{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **VEGETABLE IMAGE CLASSIFICATION**\n\nThe dataset used is [Vegetable Image Dataset](https://www.kaggle.com/datasets/misrakahmed/vegetable-image-dataset), uploaded by M Israk Ahmed on Kaggle.","metadata":{}},{"cell_type":"code","source":"# IMPORTING LIBRARIES\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.layers import *\nfrom keras.models import *\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\nimport os, shutil\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T11:03:06.05903Z","iopub.execute_input":"2022-06-30T11:03:06.059339Z","iopub.status.idle":"2022-06-30T11:03:11.499408Z","shell.execute_reply.started":"2022-06-30T11:03:06.059256Z","shell.execute_reply":"2022-06-30T11:03:11.498539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DATASET LOADING & IMAGE VISUALIZATION\n# Let's plot a few images\ntrain_path = \"../input/vegetable-image-dataset/Vegetable Images/train\"\nvalidation_path = \"../input/vegetable-image-dataset/Vegetable Images/validation\"\ntest_path = \"../input/vegetable-image-dataset/Vegetable Images/test\"\n\nimage_categories = os.listdir('../input/vegetable-image-dataset/Vegetable Images/train')\n\ndef plot_images(image_categories):\n    \n    # Create a figure\n    plt.figure(figsize=(12, 12))\n    for i, cat in enumerate(image_categories):\n        \n        # Load images for the ith category\n        image_path = train_path + '/' + cat\n        images_in_folder = os.listdir(image_path)\n        first_image_of_folder = images_in_folder[0]\n        first_image_path = image_path + '/' + first_image_of_folder\n        img = image.load_img(first_image_path)\n        img_arr = image.img_to_array(img)/255.0\n        \n        \n        # Create Subplot and plot the images\n        plt.subplot(4, 4, i+1)\n        plt.imshow(img_arr)\n        plt.title(cat)\n        plt.axis('off')\n        \n    plt.show()\n\n# Call the function\nplot_images(image_categories)\n        ","metadata":{"execution":{"iopub.status.busy":"2022-06-30T11:03:41.913963Z","iopub.execute_input":"2022-06-30T11:03:41.914238Z","iopub.status.idle":"2022-06-30T11:03:46.523912Z","shell.execute_reply.started":"2022-06-30T11:03:41.91419Z","shell.execute_reply":"2022-06-30T11:03:46.521283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IMAGE DATA GENERATOR for each set\n\n# 1. Train Set\ntrain_gen = ImageDataGenerator(rescale = 1.0/255.0) # Normalise the data\ntrain_image_generator = train_gen.flow_from_directory(\n                                            train_path,\n                                            target_size=(150, 150),\n                                            batch_size=32,\n                                            class_mode='categorical')\n\n# 2. Validation Set\nval_gen = ImageDataGenerator(rescale = 1.0/255.0) # Normalise the data\nval_image_generator = train_gen.flow_from_directory(\n                                            validation_path,\n                                            target_size=(150, 150),\n                                            batch_size=32,\n                                            class_mode='categorical')\n\n# 3. Test Set\ntest_gen = ImageDataGenerator(rescale = 1.0/255.0) # Normalise the data\ntest_image_generator = train_gen.flow_from_directory(\n                                            test_path,\n                                            target_size=(150, 150),\n                                            batch_size=32,\n                                            class_mode='categorical')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T11:03:59.19092Z","iopub.execute_input":"2022-06-30T11:03:59.191191Z","iopub.status.idle":"2022-06-30T11:04:01.529074Z","shell.execute_reply.started":"2022-06-30T11:03:59.19116Z","shell.execute_reply":"2022-06-30T11:04:01.528295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass_map = dict([(v, k) for k, v in train_image_generator.class_indices.items()])\nprint(class_map)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T11:04:04.840478Z","iopub.execute_input":"2022-06-30T11:04:04.841029Z","iopub.status.idle":"2022-06-30T11:04:04.847512Z","shell.execute_reply.started":"2022-06-30T11:04:04.840993Z","shell.execute_reply":"2022-06-30T11:04:04.846732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CNN MODEL BUILD\n\nmodel = Sequential() # model object\n\n# Add Layers\nmodel.add(Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu', input_shape=[150, 150, 3]))\nmodel.add(MaxPooling2D(2, ))\nmodel.add(Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling2D(2))\n\n# Flatten the feature map\nmodel.add(Flatten())\n\n# Add the fully connected layers\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(15, activation='softmax'))\n\n# print the model summary\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T11:04:19.242116Z","iopub.execute_input":"2022-06-30T11:04:19.24249Z","iopub.status.idle":"2022-06-30T11:04:22.542077Z","shell.execute_reply.started":"2022-06-30T11:04:19.242451Z","shell.execute_reply":"2022-06-30T11:04:22.540571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nearly_stopping = keras.callbacks.EarlyStopping(patience=5) # Set up callbacks\nmodel.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\nhist = model.fit(train_image_generator, \n                 epochs=100, \n                 verbose=1, \n                 validation_data=val_image_generator, \n                 steps_per_epoch = 15000//32, \n                 validation_steps = 3000//32, \n                 callbacks=early_stopping)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T11:04:30.54562Z","iopub.execute_input":"2022-06-30T11:04:30.54588Z","iopub.status.idle":"2022-06-30T11:06:55.596819Z","shell.execute_reply.started":"2022-06-30T11:04:30.545851Z","shell.execute_reply":"2022-06-30T11:06:55.59467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ERROR AND ACCURACY PLOT\nh = hist.history\nplt.style.use('ggplot')\nplt.figure(figsize=(12, 6))\nplt.plot(h['loss'], c='green', label='Training Loss')\nplt.plot(h['val_loss'], c='red', linestyle='--', label='Validation Loss')\nplt.plot(h['accuracy'], c='yellow', label='Training Accuracy')\nplt.plot(h['val_accuracy'], c='blue', linestyle='--', label='Validation Accuracy')\nplt.xlabel(\"Number of Epochs\")\nplt.legend(loc='best')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T11:07:21.84447Z","iopub.execute_input":"2022-06-30T11:07:21.84499Z","iopub.status.idle":"2022-06-30T11:07:21.869917Z","shell.execute_reply.started":"2022-06-30T11:07:21.844953Z","shell.execute_reply":"2022-06-30T11:07:21.869092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel.evaluate(test_image_generator)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T16:56:33.605462Z","iopub.execute_input":"2022-01-08T16:56:33.605878Z","iopub.status.idle":"2022-01-08T16:56:58.884165Z","shell.execute_reply.started":"2022-01-08T16:56:33.605841Z","shell.execute_reply":"2022-01-08T16:56:58.883491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MODEL TESTING\ntest_image_path = '../input/vegetable-image-dataset/Vegetable Images/test/Broccoli/1011.jpg'\n\ndef generate_predictions(test_image_path, actual_label):\n    \n    # 1. Load and preprocess the image\n    test_img = image.load_img(test_image_path, target_size=(150, 150))\n    test_img_arr = image.img_to_array(test_img)/255.0\n    test_img_input = test_img_arr.reshape((1, test_img_arr.shape[0], test_img_arr.shape[1], test_img_arr.shape[2]))\n\n    # 2. Make Predictions\n    predicted_label = np.argmax(model.predict(test_img_input))\n    predicted_vegetable = class_map[predicted_label]\n    plt.figure(figsize=(4, 4))\n    plt.imshow(test_img_arr)\n    plt.title(\"Predicted Label: {}, Actual Label: {}\".format(predicted_vegetable, actual_label))\n    plt.grid()\n    plt.axis('off')\n    plt.show()\n\n# call the function\ngenerate_predictions(test_image_path, actual_label='Brocoli')","metadata":{"execution":{"iopub.status.busy":"2022-01-08T17:14:14.898474Z","iopub.execute_input":"2022-01-08T17:14:14.899158Z","iopub.status.idle":"2022-01-08T17:14:15.02412Z","shell.execute_reply.started":"2022-01-08T17:14:14.89912Z","shell.execute_reply":"2022-01-08T17:14:15.023368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# EXTERNAL IMAGE PREDICTION TESTING\n!wget \"https://www.dropbox.com/s/i020rz847u8bq09/beans.jpg?dl=0\"","metadata":{"execution":{"iopub.status.busy":"2022-01-08T16:56:59.10972Z","iopub.execute_input":"2022-01-08T16:56:59.110462Z","iopub.status.idle":"2022-01-08T16:57:01.175398Z","shell.execute_reply.started":"2022-01-08T16:56:59.110424Z","shell.execute_reply":"2022-01-08T16:57:01.174644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget \"https://www.dropbox.com/s/lge1plvr4mg5w7y/potato_2.jpg?dl=0\"","metadata":{"execution":{"iopub.status.busy":"2022-01-08T16:57:01.177749Z","iopub.execute_input":"2022-01-08T16:57:01.178497Z","iopub.status.idle":"2022-01-08T16:57:03.157904Z","shell.execute_reply.started":"2022-01-08T16:57:01.17845Z","shell.execute_reply":"2022-01-08T16:57:03.157098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nexternal_image_path_1 = \"./beans.jpg?dl=0\"\ngenerate_predictions(external_image_path_1, actual_label='Bean')","metadata":{"execution":{"iopub.status.busy":"2022-01-08T16:57:03.159721Z","iopub.execute_input":"2022-01-08T16:57:03.160009Z","iopub.status.idle":"2022-01-08T16:57:03.304377Z","shell.execute_reply.started":"2022-01-08T16:57:03.159964Z","shell.execute_reply":"2022-01-08T16:57:03.303667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nexternal_image_path_2 = \"./potato_2.jpg?dl=0\"\ngenerate_predictions(external_image_path_2, actual_label='Potato')","metadata":{"execution":{"iopub.status.busy":"2022-01-08T16:57:03.305799Z","iopub.execute_input":"2022-01-08T16:57:03.306156Z","iopub.status.idle":"2022-01-08T16:57:03.418323Z","shell.execute_reply.started":"2022-01-08T16:57:03.306123Z","shell.execute_reply":"2022-01-08T16:57:03.417621Z"},"trusted":true},"execution_count":null,"outputs":[]}]}