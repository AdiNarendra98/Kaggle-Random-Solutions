{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h3><center>üòÅFaceNetüòä</center></h3>\n","metadata":{"execution":{"iopub.status.busy":"2022-07-16T07:17:31.365114Z","iopub.execute_input":"2022-07-16T07:17:31.365515Z","iopub.status.idle":"2022-07-16T07:17:31.371821Z","shell.execute_reply.started":"2022-07-16T07:17:31.365483Z","shell.execute_reply":"2022-07-16T07:17:31.370699Z"}}},{"cell_type":"markdown","source":"* The Dataset used is [5 Celebrity Faces Dataset](https://www.kaggle.com/datasets/dansbecker/5-celebrity-faces-dataset),uploaded by DanB on Kaggle.","metadata":{}},{"cell_type":"markdown","source":"<h3><center>Introduction</center></h3>\n\n\n\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\nFaceNet is a face recognition system developed in 2015 by researchers at Google that achieved then state-of-the-art results on a range of face recognition benchmark datasets.<br><br>    \nThe FaceNet system can be used to extract high-quality features from faces(128 element vector representation), called face embeddings, that can then be used to train a face identification system. In this kernel, we will  develop a face detection system using FaceNet and an SVM classifier to identify people from photographs.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<h3><center>1. Importing Libraries</center></h3>","metadata":{}},{"cell_type":"code","source":"!pip install -q mtcnn","metadata":{"execution":{"iopub.status.busy":"2022-07-16T07:14:46.545745Z","iopub.execute_input":"2022-07-16T07:14:46.546201Z","iopub.status.idle":"2022-07-16T07:14:55.751410Z","shell.execute_reply.started":"2022-07-16T07:14:46.546112Z","shell.execute_reply":"2022-07-16T07:14:55.750499Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport random\n\nfrom tqdm import tqdm_notebook as tqdm\ntqdm().pandas()\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.svm import SVC\n\nfrom mtcnn.mtcnn import MTCNN","metadata":{"execution":{"iopub.status.busy":"2022-07-16T07:14:59.887558Z","iopub.execute_input":"2022-07-16T07:14:59.887988Z","iopub.status.idle":"2022-07-16T07:15:07.693384Z","shell.execute_reply.started":"2022-07-16T07:14:59.887942Z","shell.execute_reply":"2022-07-16T07:15:07.692267Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"PATH = '/kaggle/input/5-celebrity-faces-dataset/train/ben_afflek/'\n\nROOT_PATH = '/kaggle/input/5-celebrity-faces-dataset/'\n\nTRAIN_PATH = '/kaggle/input/5-celebrity-faces-dataset/train/'\n\nVAL_PATH = '/kaggle/input/5-celebrity-faces-dataset/val/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nfor i, filename in enumerate(os.listdir(PATH)):\n    path = PATH + filename\n    image = plt.imread(path)\n    \n    plt.subplot(2, 7, i+1)\n    \n    plt.axis('off')\n    plt.imshow(image)\n    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><center>2. Detect Faces using MTCNN</center></h3>","metadata":{}},{"cell_type":"code","source":"# extract a single face from a given photograph\ndef extract_face(filename, required_size=(160, 160)):\n    image = Image.open(filename)\n    image = image.convert('RGB')\n    pixels = np.asarray(image)\n    \n    detector = MTCNN()\n    results = detector.detect_faces(pixels)\n    \n    # extract the bounding box from the first face\n    x1, y1, width, height = results[0]['box']\n    # bug fix\n    x1, y1 = abs(x1), abs(y1)\n    \n    x2, y2 = x1 + width, y1 + height\n    \n    face = pixels[y1:y2, x1:x2]\n    \n    image = Image.fromarray(face)\n    \n    image = image.resize(required_size)\n    face_array = np.asarray(image)\n    \n    return face_array\n\nplt.figure(figsize=(10,5))\nfor i, filename in tqdm(enumerate(os.listdir(PATH))):\n    path = PATH + filename\n    \n    face = extract_face(path)\n    \n    #print(i+1, face.shape)\n    \n    plt.subplot(2, 7, i+1)\n    \n    plt.axis('off')\n    plt.imshow(face)\n    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><center>3. Load Faces</center></h3>","metadata":{}},{"cell_type":"code","source":"def load_faces(directory):\n    faces = list()\n    for file_name in tqdm(os.listdir(directory)):\n        path = directory + file_name\n        face = extract_face(path)\n        \n        faces.append(face)\n    return faces\n\ndef load_dataset(directory):\n    images, labels = list(), list()\n    for folder in tqdm(os.listdir(directory)):\n        path = directory + folder + '/'\n        \n        if not os.path.isdir(path):\n            continue\n        \n        faces = load_faces(path)\n\n        print(f'Celebrity : {folder}, Faces : {len(faces)}')\n\n        label = [folder for _ in range(len(faces))]\n\n        images.extend(faces)\n        labels.extend(label)\n        \n    return np.asarray(images), np.asarray(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, y_train = load_dataset(TRAIN_PATH)\n\nX_test, y_test = load_dataset(VAL_PATH)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><center>4. Create Face Embeddings</center></h3>\n\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\n    A face embedding is a vector that represents the features extracted from the face.<br>\n    For example, another vector that is close (by some measure) may be the same person, whereas another vector that is far (by some measure) may be a different person. The classifier model that we want to develop will take a face embedding as input and predict the identity of the face. The FaceNet model will generate this embedding for a given image of a face.</div>","metadata":{}},{"cell_type":"code","source":"face_pixels = X_train[0]\n\nprint(face_pixels.shape)\n\nface_pixels = np.expand_dims(face_pixels, axis=0)\nprint(face_pixels.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\n\ndef get_embedding(model, face_pixels):\n    face_pixels = face_pixels.astype('float32')\n    \n    # standardize pixel values across channels (global) \n    mean, std = face_pixels.mean(), face_pixels.std() \n    face_pixels = (face_pixels - mean) / std\n    \n    samples = np.expand_dims(face_pixels, axis=0)\n    \n    yhat = model.predict(samples)\n    \n    return yhat[0]\n\nmodel = load_model('/kaggle/input/keras-facenet/facenet_keras.h5')\n\n\ntrainX = list()\n\nfor pixels in tqdm(X_train):\n    embedding = get_embedding(model, pixels)\n    trainX.append(embedding)\ntrainX = np.asarray(trainX)\n\nprint('Train X :', trainX.shape)\n\n\ntestX = list()\n\nfor pixels in tqdm(X_test):\n    embedding = get_embedding(model, pixels)\n    testX.append(embedding)\ntestX = np.asarray(testX)\n\nprint('test X :', testX.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><center>5. Perform Face Classification</center></h3>","metadata":{}},{"cell_type":"code","source":"norm = Normalizer(norm='l2')\n\ntrainX = norm.transform(trainX)\ntestX = norm.transform(testX)\n\nlabel = LabelEncoder()\n\ntrainy = label.fit_transform(y_train)\ntesty = label.fit_transform(y_test)\n\nmodel = SVC(kernel='linear', probability=True)\nmodel.fit(trainX, trainy)\n\nyhat_train = model.predict(trainX)\nyhat_test = model.predict(testX)\n\nscore_train = accuracy_score(trainy, yhat_train)\nscore_test = accuracy_score(testy, yhat_test)\n\nprint('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Choose a random index\nrandom.seed(100)\nselection = random.choice([i for i in range(testX.shape[0])])\n\nface = X_test[selection]\n\nyhat_prob = model.predict_proba(testX[selection].reshape(1,-1))\nyhat = model.predict(testX[selection].reshape(1,-1))\n\nceleb = label.inverse_transform(yhat)\n\nplt.imshow(face)\nplt.axis('off')\n\nprint(f'Probability : {np.max(yhat_prob)*100}, \\nCelebrity - Predicted : {celeb}, Acutal : {y_test[selection]}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><center>6. T-SNE Plot </center></h3>","metadata":{}},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport seaborn as sns\n\ntsne = TSNE(learning_rate=100)\n\ntsne_features = tsne.fit_transform(trainX)\n\nX = tsne_features[:,0]\ny = tsne_features[:,1]\n\ndataset = pd.DataFrame(data=y_train, columns=['label'])\ndataset['X'] = X\ndataset['y'] = y\n\nplt.figure(figsize=(13,8))\nsns.scatterplot(data=dataset, x='X', y='y', hue='label', s=120)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}